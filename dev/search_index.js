var documenterSearchIndex = {"docs":
[{"location":"functions/#Basic-Functions","page":"Basic Functions","title":"Basic Functions","text":"","category":"section"},{"location":"functions/","page":"Basic Functions","title":"Basic Functions","text":"CurrentModule = Downhill","category":"page"},{"location":"functions/","page":"Basic Functions","title":"Basic Functions","text":"optimize\n\noptimize!\n\nsolver","category":"page"},{"location":"functions/#Downhill.optimize","page":"Basic Functions","title":"Downhill.optimize","text":"optimize(\n    fdf, x₀;\n    method,\n    kw...\n)\n\nFind an optimizer for fdf, starting with the initial approximation x₀.     method keyword chooses a specific optimization method. See optimize! for     the description of other keywords.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Downhill.optimize!","page":"Basic Functions","title":"Downhill.optimize!","text":"optimize!(fdf, M::Wrapper, x0)\n\nFind an optimizer for fdf, starting with the initial approximation x0. fdf(x, g) must return a tuple (f(x), ∇f(x)) and, if g is mutable, overwrite it with the gradient.\n\n\n\n\n\noptimize!(\n    fdf, M::OptBuffer, x₀;\n    gtol=1e-6,\n    convcond=nothing,\n    maxiter=100,\n    maxcalls=nothing,\n    reset=true,\n    constrain_step=nothing,\n    tracking=nothing,\n    verbosity=0\n)\n\nFind an optimizer for fdf, starting with the initial approximation x₀.\n\nArguments\n\nM::OptBuffer: the core method to use for optimization\nfdf(x, g)::Function: function to optimize. It must return a tuple (f(x), ∇f(x)) and,   if g is mutable, overwrite   it with the gradient.\nx0: initial approximation\n\nKeywords\n\nConvergence criteria\n\nThere are two options to specify convergence criterion. The default is by gtol and the second by custom stop convcond.\n\ngtol::Real: (default stop criterion) stop optimization when the gradient's 2-norm is less\nconvcond=(x, xpre, y, ypre, g)->Bool: function, custom stop criterion based on argument   values, function values and gradient. If nothing (default), corresponds to gtol,   and when specified, the gtol-criterion is ignored.\n\nExample (default criterion): convcond = (x, xpre, y, ypre, g) -> norm(g, 2) ≤ gtol.\n\nLimitting optimization\n\n(Un)Limit optimization process by specifing either maxiter and/or maxcalls.\n\nmaxiter::Integer: force stop optimization after this number of iterations   (use nothing or a negative value to not constrain iteration number)\nmaxcalls::Integer: force stop optimization after this number of function calls   (use nothing or a negative value to not constrain call number)\n\nOptimization constrains\n\nThe inequality constrains of optimization is handled by constrain_step.\n\nconstrain_step(x0, d): a function to constrain step from x0 in the direction d.   It must return a real-numbered value α such that x0 + αd is the maximum allowed step\n\nInitializing\n\nreset=true: a value to pass as a keyword argument to the optimizer init! method\n\nOptimization path\n\ntracking::Union{Nothing,IO,AbstractString}: IO stream or a file name to log the   optimization process or nothing to disable logging (default: nothing)\nverbosity::Integer: verbosity of logging. 0 (default) disables tracking. 1 logs all   points of objective function evaluation with corresponding values and gradients.   2 shows additional statistics regarding the line search. Option ignored if   tracking == nothing.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Downhill.solver","page":"Basic Functions","title":"Downhill.solver","text":"Downhill.solver(\n    M::OptBuffer;\n    gtol = 1e-6,\n    maxiter = 100,\n    maxcalls = nothing,\n    constrain_step=nothing,\n)\n\nReturn the wrapper object for a chosen method to solve an optimization problem with given     parameters. For the description of keywords, see optimize!\n\n\n\n\n\nDownhill.solver(\n    M::DataType, x;\n    gtol=1e-6, maxiter = 100, maxcalls = nothing, constrain_step)\n\nReturn the wrapper object for a chosen method to solve an optimization problem with given     parameters compatible with the dimensions of x.\n\n\n\n\n\n","category":"function"},{"location":"wrappers/#Customization","page":"Customization","title":"Customization","text":"","category":"section"},{"location":"wrappers/","page":"Customization","title":"Customization","text":"CurrentModule = Downhill","category":"page"},{"location":"wrappers/","page":"Customization","title":"Customization","text":"Subtypes of Downhill.Wrapper are used to add custom behavior to optimization,  such as stopping after reaching certain criteria, limiting timesteps etc.","category":"page"},{"location":"wrappers/","page":"Customization","title":"Customization","text":"A wrapper around an object of AbstractOptBuffer type should at a minimum provide  the base_method(M::Wrapper) to return the wrapped object.","category":"page"},{"location":"wrappers/","page":"Customization","title":"Customization","text":"The wrappers can modify the optimization behavior by overloading the following methods:","category":"page"},{"location":"wrappers/","page":"Customization","title":"Customization","text":"init!\n\ncallfn!\n\nreset!\n\nstep!\n\nstopcond\n\nconv_success","category":"page"},{"location":"wrappers/#Downhill.init!","page":"Customization","title":"Downhill.init!","text":"optfn! must be the 3-arg closure that computes fdf(x + α*d) and overwrites M's gradient\n\n\n\n\n\n","category":"function"},{"location":"wrappers/#Downhill.callfn!","page":"Customization","title":"Downhill.callfn!","text":"callfn!(fdf, M::AbstractOptBuffer, x, α, d)\n\nReturn the value of fdf(x + α * d) overwriting the internal buffers of M as needed.     fdf(x, g) must return a tuple (f(x), ∇f(x)) and, if g is mutable, overwrite     it with the gradient (see optimize!).\n\n\n\n\n\n","category":"function"},{"location":"wrappers/#Downhill.reset!","page":"Customization","title":"Downhill.reset!","text":"reset!(M::AbstractOptBuffer, args...; kwargs...)\n\nReset the solver parameters to the default (or to specific value – see the documentation     for the specific types).\n\nEach method has to implement a parameter-free reset!(M) method.\n\n\n\n\n\n","category":"function"},{"location":"wrappers/#Downhill.step!","page":"Customization","title":"Downhill.step!","text":"step!(fdf, M::AbstractOptBuffer; kw...)\n\nMake one iteration of optimization routine from the current state of M using fdf as the     objective function and modifying the internal buffers as needed.\n\n\n\n\n\n","category":"function"},{"location":"wrappers/#Downhill.stopcond","page":"Customization","title":"Downhill.stopcond","text":"stopcond(M::AbstractOptBuffer)\n\nDecide if the optimization should be stopped from the state of M.\n\n\n\n\n\n","category":"function"},{"location":"wrappers/#Downhill.conv_success","page":"Customization","title":"Downhill.conv_success","text":"conv_success(M::AbstractOptBuffer)\n\nDecide if the optimization has converged from the state of M (useful to distinguish     between stopping by convergence and stopping by iterations count when the convergence     has been achieved on the limiting iteration).\n\n\n\n\n\n","category":"function"},{"location":"wrappers/#Predefined-Wrappers","page":"Customization","title":"Predefined Wrappers","text":"","category":"section"},{"location":"wrappers/","page":"Customization","title":"Customization","text":"BasicConvergenceStats\n\nConstrainStepSize","category":"page"},{"location":"wrappers/#Downhill.BasicConvergenceStats","page":"Customization","title":"Downhill.BasicConvergenceStats","text":"BasicConvergenceStats\n\nWrapper type to provide basic stop conditions: magnitude of gradient is less than the     specified value, objective function call count exceeds threshold, iteration count     exceeds threshold.\n\n\n\n\n\n","category":"type"},{"location":"wrappers/#Downhill.ConstrainStepSize","page":"Customization","title":"Downhill.ConstrainStepSize","text":"ConstrainStepSize\n\nWrapper type to limit step sizes attempted in optimization, given a function (origin, direction) -> max_step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Optimization-Methods","page":"Optimization Methods","title":"Optimization Methods","text":"","category":"section"},{"location":"core_types/","page":"Optimization Methods","title":"Optimization Methods","text":"Optimization methods are defined as subtypes AbstractOptBuffer type.  The structs holding the data required for core iteration logic should subtype   OptBuffer <: AbstractOptBuffer.  Stopping criteria, logging, step limitations etc. are implemented as subtypes of  Wrapper <: AbstractOptBuffer.","category":"page"},{"location":"core_types/#Core-Optimization-Methods","page":"Optimization Methods","title":"Core Optimization Methods","text":"","category":"section"},{"location":"core_types/","page":"Optimization Methods","title":"Optimization Methods","text":"FixedRateDescent\n\nMomentumDescent\n\nNesterovMomentum\n\nSteepestDescent\n\nHyperGradDescent\n\nCGDescent\n\nBFGS\n\nCholBFGS","category":"page"},{"location":"core_types/#Downhill.FixedRateDescent","page":"Optimization Methods","title":"Downhill.FixedRateDescent","text":"FixedRateDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.MomentumDescent","page":"Optimization Methods","title":"Downhill.MomentumDescent","text":"MomentumDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.NesterovMomentum","page":"Optimization Methods","title":"Downhill.NesterovMomentum","text":"NesterovMomentum\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.SteepestDescent","page":"Optimization Methods","title":"Downhill.SteepestDescent","text":"SteepestDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.HyperGradDescent","page":"Optimization Methods","title":"Downhill.HyperGradDescent","text":"HyperGradDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.CGDescent","page":"Optimization Methods","title":"Downhill.CGDescent","text":"CGDescent\n\nConjugate gradient method (Hager-Zhang version [W.Hager, H.Zhang // SIAM J. Optim (2006) Vol. 16, pp. 170-192])\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.BFGS","page":"Optimization Methods","title":"Downhill.BFGS","text":"BFGS\n\nQuasi-Newton descent method.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.CholBFGS","page":"Optimization Methods","title":"Downhill.CholBFGS","text":"CholBFGS\n\nQuasi-Newton descent method.\n\n\n\n\n\n","category":"type"},{"location":"#Downhill.jl","page":"Downhill.jl","title":"Downhill.jl","text":"","category":"section"},{"location":"","page":"Downhill.jl","title":"Downhill.jl","text":"CurrentModule = Downhill","category":"page"},{"location":"","page":"Downhill.jl","title":"Downhill.jl","text":"A collection of descent-based optimization methods.","category":"page"},{"location":"","page":"Downhill.jl","title":"Downhill.jl","text":"The package is meant to be used for small-scale optimization problems.  The use case is the problems where an optimization is some intermediate step  that has to be run repeatedly.","category":"page"},{"location":"#Basic-usage","page":"Downhill.jl","title":"Basic usage","text":"","category":"section"},{"location":"","page":"Downhill.jl","title":"Downhill.jl","text":"julia> function rosenbrock!(x::AbstractVector, g::AbstractVector; b=100)\n           f = zero(eltype(g))\n           fill!(g, 0)\n           inds = eachindex(x, g)\n           for i in 2:last(inds)\n               f += (1 - x[i-1])^2 + b * (x[i] - x[i-1]^2)^2\n               g[i-1] += 2 * (x[i-1] - 1) + 4 * b * x[i-1] * (x[i-1]^2 - x[i])\n               g[i] += 2 * b * (x[i] - x[i-1]^2)\n           end\n           return f, g\n       end\n\njulia> let x0 = zeros(2)\n           opt = BFGS(x0)\n           optresult = optimize!(rosenbrock!, opt, x0)\n           optresult.argument\n       end\n2-element Vector{Float64}:\n 0.9999999998907124\n 0.9999999998080589","category":"page"}]
}
