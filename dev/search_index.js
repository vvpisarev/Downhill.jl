var documenterSearchIndex = {"docs":
[{"location":"functions/#Basic-Functions","page":"Basic Functions","title":"Basic Functions","text":"","category":"section"},{"location":"functions/","page":"Basic Functions","title":"Basic Functions","text":"CurrentModule = Downhill","category":"page"},{"location":"functions/","page":"Basic Functions","title":"Basic Functions","text":"optimize\n\noptimize!\n\nsolver","category":"page"},{"location":"functions/#Downhill.optimize","page":"Basic Functions","title":"Downhill.optimize","text":"optimize(\n    fdf, x₀;\n    method,\n    kw...\n)\n\nFind an optimizer for fdf, starting with the initial approximation x₀.     method keyword chooses a specific optimization method. See optimize! for     the description of other keywords.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Downhill.optimize!","page":"Basic Functions","title":"Downhill.optimize!","text":"optimize!(fdf, M::Wrapper, x0)\n\nFind an optimizer for fdf, starting with the initial approximation x0. fdf(x, g) must return a tuple (f(x), ∇f(x)) and, if g is mutable, overwrite it with the gradient.\n\n\n\n\n\noptimize!(\n    fdf, M::OptBuffer, x₀;\n    gtol=1e-6,\n    maxiter=100,\n    maxcalls=nothing,\n    reset=true,\n    constrain_step=nothing,\n    tracking=stdout,\n    verbosity=0\n)\n\nFind an optimizer for fdf, starting with the initial approximation x₀.\n\nArguments:\n\nM::OptBuffer: the core method to use for optimization\nfdf(x, g)::Function: function to optimize. It must return a tuple (f(x), ∇f(x)) and,   if g is mutable, overwrite   it with the gradient.\nx0: initial approximation\n\nKeywords:\n\ngtol::Real: stop optimization when the gradient norm is less\nmaxiter::Integer: force stop optimization after this number of iterations   (use nothing or a negative value to not constrain iteration number)\nmaxcalls::Integer: force stop optimization after this number of function calls   (use nothing or a negative value to not constrain call number)\nreset=true: a value to pass as a keyword argument to the optimizer init! method\nconstrain_step(x0, d): a function to constrain step from x0 in the direction d.   It must return a real-numbered value α such that x0 + αd is the maximum allowed step\ntracking::Union{Nothing,IO,AbstractString}: IO stream or a file name to log the   optimization process or nothing to disable logging (default: nothing)\nverbosity::Integer: verbosity of logging. 0 (default) disables tracking. 1 logs all   points of objective function evaluation with corresponding values and gradients.   2 shows additional statistics regarding the line search. Option ignored if   tracking == nothing.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Downhill.solver","page":"Basic Functions","title":"Downhill.solver","text":"Downhill.solver(\n    M::OptBuffer;\n    gtol = 1e-6,\n    maxiter = 100,\n    maxcalls = nothing,\n    constrain_step=nothing,\n)\n\nReturn the wrapper object for a chosen method to solve an optimization problem with given     parameters. For the description of keywords, see optimize!\n\n\n\n\n\nDownhill.solver(\n    M::DataType, x;\n    gtol=1e-6, maxiter = 100, maxcalls = nothing, constrain_step)\n\nReturn the wrapper object for a chosen method to solve an optimization problem with given     parameters compatible with the dimensions of x.\n\n\n\n\n\n","category":"function"},{"location":"core_types/#Exported-Optimization-Methods","page":"Optimization Methods","title":"Exported Optimization Methods","text":"","category":"section"},{"location":"core_types/","page":"Optimization Methods","title":"Optimization Methods","text":"FixedRateDescent\n\nMomentumDescent\n\nNesterovMomentum\n\nSteepestDescent\n\nHyperGradDescent\n\nCGDescent\n\nBFGS\n\nCholBFGS","category":"page"},{"location":"core_types/#Downhill.FixedRateDescent","page":"Optimization Methods","title":"Downhill.FixedRateDescent","text":"FixedRateDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.MomentumDescent","page":"Optimization Methods","title":"Downhill.MomentumDescent","text":"MomentumDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.NesterovMomentum","page":"Optimization Methods","title":"Downhill.NesterovMomentum","text":"NesterovMomentum\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.SteepestDescent","page":"Optimization Methods","title":"Downhill.SteepestDescent","text":"SteepestDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.HyperGradDescent","page":"Optimization Methods","title":"Downhill.HyperGradDescent","text":"HyperGradDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.CGDescent","page":"Optimization Methods","title":"Downhill.CGDescent","text":"CGDescent\n\nConjugate gradient method (Hager-Zhang version [W.Hager, H.Zhang // SIAM J. Optim (2006) Vol. 16, pp. 170-192])\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.BFGS","page":"Optimization Methods","title":"Downhill.BFGS","text":"BFGS\n\nQuasi-Newton descent method.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#Downhill.CholBFGS","page":"Optimization Methods","title":"Downhill.CholBFGS","text":"CholBFGS\n\nQuasi-Newton descent method.\n\n\n\n\n\n","category":"type"},{"location":"#Downhill.jl","page":"Downhill.jl","title":"Downhill.jl","text":"","category":"section"},{"location":"","page":"Downhill.jl","title":"Downhill.jl","text":"CurrentModule = Downhill","category":"page"},{"location":"","page":"Downhill.jl","title":"Downhill.jl","text":"A collection of descent-based optimization methods.","category":"page"},{"location":"","page":"Downhill.jl","title":"Downhill.jl","text":"The package is meant to be used for small-scale optimization problems.  The use case is the problems where an optimization is some intermediate step  that has to be run repeatedly.","category":"page"},{"location":"#Basic-usage","page":"Downhill.jl","title":"Basic usage","text":"","category":"section"},{"location":"","page":"Downhill.jl","title":"Downhill.jl","text":"julia> function rosenbrock!(x::AbstractVector, g::AbstractVector; b=100)\n           f = zero(eltype(g))\n           fill!(g, 0)\n           inds = eachindex(x, g)\n           for i in 2:last(inds)\n               f += (1 - x[i-1])^2 + b * (x[i] - x[i-1]^2)^2\n               g[i-1] += 2 * (x[i-1] - 1) + 4 * b * x[i-1] * (x[i-1]^2 - x[i])\n               g[i] += 2 * b * (x[i] - x[i-1]^2)\n           end\n           return f, g\n       end\n\njulia> let x0 = zeros(2)\n           opt = BFGS(x0)\n           optresult = optimize!(rosenbrock!, opt, x0)\n           optresult.argument\n       end\n2-element Vector{Float64}:\n 0.9999999998907124\n 0.9999999998080589","category":"page"}]
}
