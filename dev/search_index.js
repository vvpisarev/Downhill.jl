var documenterSearchIndex = {"docs":
[{"location":"functions/#Basic-Functions","page":"Basic Functions","title":"Basic Functions","text":"","category":"section"},{"location":"functions/","page":"Basic Functions","title":"Basic Functions","text":"CurrentModule = DescentMethods","category":"page"},{"location":"functions/","page":"Basic Functions","title":"Basic Functions","text":"optimize!\n\nsolver\n\nreset!","category":"page"},{"location":"functions/#DescentMethods.optimize!","page":"Basic Functions","title":"DescentMethods.optimize!","text":"optimize!(M::Wrapper, fdf, x0)\n\nFind an optimizer for fdf, starting with the initial approximation x0. fdf(x, g) must return a tuple (f(x), ∇f(x)) and, if g is mutable, overwrite it with the gradient.\n\n\n\n\n\noptimize!(\n    M::CoreMethod, fdf, x0;\n    gtol=1e-6,\n    maxiter=100,\n    maxcalls=nothing,\n    reset=true,\n    constrain_step=nothing,\n    log_stream=nothing,\n    verbosity=1\n)\n\nFind an optimizer for fdf, starting with the initial approximation x0.\n\nArguments:\n\nM::CoreMethod: the core method to use for optimization\nfdf(x, g)::Function: function to optimize. It must return a tuple (f(x), ∇f(x)) and,   if g is mutable, overwrite   it with the gradient.\nx0: initial approximation\n\nKeywords:\n\ngtol::Real: stop optimization when the gradient norm is less\nmaxiter::Integer: force stop optimization after this number of iterations   (use nothing or a negative value to not constrain iteration number)\nmaxcalls::Integer: force stop optimization after this number of function calls   (use nothing or a negative value to not constrain call number)\nreset=true: a value to pass as a keyword argument to the optimizer init! method\nconstrain_step(x0, d): a function to constrain step from x0 in the direction d.   It must return a real-numbered value α such that x0 + αd is the maximum allowed step\nlog_stream::Union{IO,AbstractString,Nothing}: IO stream or a file name to log the   optimization process or nothing to disable logging (default: nothing)\nverbosity::Integer=1: verbosity of logging. 1 (default) logs all points of objective   function evaluation with corresponding values and gradients. 2 shows additional   statistics regarding the line search.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DescentMethods.solver","page":"Basic Functions","title":"DescentMethods.solver","text":"DescentMethods.solver(\n    M::CoreMethod;\n    gtol = 1e-6,\n    maxiter = 100,\n    maxcalls = nothing,\n    constrain_step=nothing,\n)\n\nReturn the wrapper object for a chosen method to solve an optimization problem with given     parameters. For the description of keywords, see optimize!\n\n\n\n\n\nDescentMethods.solver(\n    M::DataType, x;\n    gtol=1e-6, maxiter = 100, maxcalls = nothing, constrain_step)\n\nReturn the wrapper object for a chosen method to solve an optimization problem with given     parameters compatible with the dimensions of x.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DescentMethods.reset!","page":"Basic Functions","title":"DescentMethods.reset!","text":"reset!(M::DescentMethod, args...; kwargs...)\n\nReset the solver parameters to the default (or to specific value - see the documentation for     the specific types).\n\nEach method has to implement a parameter-free reset!(M) method.\n\n\n\n\n\n","category":"function"},{"location":"core_types/#Exported-Optimization-Methods","page":"Optimization Methods","title":"Exported Optimization Methods","text":"","category":"section"},{"location":"core_types/","page":"Optimization Methods","title":"Optimization Methods","text":"FixedRateDescent\n\nMomentumDescent\n\nNesterovMomentum\n\nSteepestDescent\n\nHyperGradDescent\n\nCGDescent\n\nBFGS\n\nCholBFGS","category":"page"},{"location":"core_types/#DescentMethods.FixedRateDescent","page":"Optimization Methods","title":"DescentMethods.FixedRateDescent","text":"FixedRateDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#DescentMethods.MomentumDescent","page":"Optimization Methods","title":"DescentMethods.MomentumDescent","text":"MomentumDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#DescentMethods.NesterovMomentum","page":"Optimization Methods","title":"DescentMethods.NesterovMomentum","text":"NesterovMomentum\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#DescentMethods.SteepestDescent","page":"Optimization Methods","title":"DescentMethods.SteepestDescent","text":"SteepestDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#DescentMethods.HyperGradDescent","page":"Optimization Methods","title":"DescentMethods.HyperGradDescent","text":"HyperGradDescent\n\nDescent method which minimizes the objective function in the direction of antigradient at each step.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#DescentMethods.CGDescent","page":"Optimization Methods","title":"DescentMethods.CGDescent","text":"CGDescent\n\nConjugate gradient method (Hager-Zhang version [W.Hager, H.Zhang // SIAM J. Optim (2006) Vol. 16, pp. 170-192])\n\n\n\n\n\n","category":"type"},{"location":"core_types/#DescentMethods.BFGS","page":"Optimization Methods","title":"DescentMethods.BFGS","text":"BFGS\n\nQuasi-Newton descent method.\n\n\n\n\n\n","category":"type"},{"location":"core_types/#DescentMethods.CholBFGS","page":"Optimization Methods","title":"DescentMethods.CholBFGS","text":"CholBFGS\n\nQuasi-Newton descent method.\n\n\n\n\n\n","category":"type"},{"location":"#DescentMethods.jl","page":"DescentMethods.jl","title":"DescentMethods.jl","text":"","category":"section"},{"location":"","page":"DescentMethods.jl","title":"DescentMethods.jl","text":"CurrentModule = DescentMethods","category":"page"}]
}
